{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Air Quality with Amazon SageMaker and DeepAR\n",
    "In this example, we are going to build an air quality forecasting application using Amazon SageMaker and the DeepAR algorithm. We will walk through how to define the problem, engineer the features, and train, evaluate and deploy the machine learning model. \n",
    "\n",
    "## Why is Air Quality important?\n",
    "Recent bush fire events in Australia is just one reminder of the importance of breathable air. According to the World Health Organization, air pollution is the 4th largest risk factor to human health worldwide, and 90% of the world breathes unhealthy air. Having realiable projections of air quality can help individuals as well as organizations to take steps to mitigate the health affects caused by dangerous air quality levels. For more information one non-profit's efforts to solve this global issue, visit [Open AQ](https://openaq.org/).\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/syd_harb_air.jpg\" width=\"600px\" alt=\"Sydney Harbour Air During Bushfires\"/>\n",
    "<figcaption>Sydney Harbour during hazardous air quality conditions</figcaption>\n",
    "</figure>\n",
    "\n",
    "## What is time series analysis?\n",
    "Time series analysis applies mathematical techniques to quantities that are ordered by time, in order to find insights about the past as well as the future. Historically, weather forecasting is one of the first time series analysis problem undertaken by humans. A early as neolithic times, civilizations used calendars as a means to predict seasonal patterns for agriculture. Time series problems exist in almost every domain. Predicting demand for future sales and services, forecasting utilization of compute resources and projecting call volume in call centers are all good examples of time series problems. Many of the methodologies used for time series analysis have been around for a long time. Long before deep learning techniques where even invented, algorithms like ARIMA have been in use since the 1950's. For many problems, these well developed techniques are still the best way to solve time series problems. The recent exponential growth in data and compute power has spawned the development of machine learning techniques based on neural networks that work well with larger and more complex data sets. One example of this type of data is air quality measurements, which are composed of millions of measurements from hundreds of locations. The DeepAR algorithm developed by Amazon research scientists to do time series forecasting on large data sets of related time series, makes training a forecasting model with this complex data possible.\n",
    "\n",
    ">  **Definitions:** Univariate means a single value type. Multivariate means multiple value types. For example, a time series of temperature and humidity is multivariate, whereas a time series of temperature alone is univariate. Many time series algorithms only work with univariate date. Some algorithms work with multivariate data, but only predicts values for a single target value type. The other time series is called the related time series or the \"exogenous\" time series. The DeepAR algorithm that we will use in this example works with multivariate data, but we will only use univariate air quality data. To improve the quality of the predictive model, we could also use an exogenous time series, such as wind or temparature, but this out of scope for this project.\n",
    "\n",
    "## Problem definition\n",
    "The first step in any machine learning problem, is to understand the desired outcome.  You need to have a concrete goal to work towards through the entire process of discovery, design, development, deployment and operation. Try to answer these questions up front:\n",
    "\n",
    "- Who will be consuming the predictions?\n",
    "- How will the predictions be used? \n",
    "- How often do predictions need to be made?\n",
    "- What are the minimum KPI's for the predictions in order for them to be useful?\n",
    "\n",
    "Throughout your project, you should continously review that your work is meeting the end goals. Defining a clear problem statement up front is critical throughout the process. \n",
    "\n",
    "For this problem, the project sponsors have given a detailed use case description:\n",
    "> *A state environmental protection agency wishes to provide air pollution estimates for particulate matter via a public web page. Predictions need to be made for over 100 locations throughout Australia. The forecast should be updated every hour, and needs to show the projected particulate matter 10 micron (pm10) values for the next 2 days on an hourly basis. A range of possible values should be shown. The generated static web page should have a link to a csv file of the predictions as well as a summary visualization. The predicted air quality should be averaged over 24 hours and classified according to the Victorian Air Quality standards for pm10. Healthy and unhealthy air days should be predicted correctly 75% of the time.*\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Air quality category</th>\n",
    "            <th>\n",
    "            <p><span>PM</span><sub><span>10</span></sub><span>&nbsp;µg/m</span><sup><span>3&nbsp;</span></sup><span>averaged over 1&nbsp;hour&nbsp;</span></p>\n",
    "            </th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"background-color: #64a13c; text-align: center;\"><span style=\"color: white;\">Good</span></th>\n",
    "            <td>&nbsp; Less than 40</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"background-color: #eac51c; text-align: center;\">Moderate</th>\n",
    "            <td>&nbsp; 40–80</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"background-color: #d67900; text-align: center;\"><span style=\"color: white;\">Poor</span></th>\n",
    "            <td>&nbsp; 80–120</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"background-color: #a90737; text-align: center;\"><span style=\"color: white;\">   Very poor</span></th>\n",
    "            <td>&nbsp; 120–240</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"background-color: #50051e; text-align: center;\"><span style=\"color: white;\">Hazardous</span></th>\n",
    "            <td>&nbsp; More than 240</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Getting such a well defined problem statement is rarely easy. You will need to spend time with the project sponsors to understand what the desired end result is, and how this translates into machine learning requirements. Despite the difficulty in establishing thorough requirements, it's critical we figure this out up front to avoid developing a machine learning model that does not meet clearly defined objectives. \n",
    "\n",
    "> There is one thing that is not clear from the problem statement above. What is the mathematical way to express \"healthy\" versus \"unhealthy\"? After speaking with the project sponsors, we are told that any pm10 value > 80 is unhealthy.\n",
    "\n",
    "From the problem statement there are several key points that determine what needs to be built:\n",
    "- This is a 2 day forecast, with hourly frequency.\n",
    "- The forecast will be created every hour using batch techniques.\n",
    "- The probability distribution of predicted pm10 values is needed, not just a point forecast.\n",
    "- We need to classify a 24 hour rolling average, according to the stated pm10 ranges for \"good\", \"moderate\", \"poor\", \"very poor\" and \"hazardous\".\n",
    "- For evaluation, the percentage of time we correctly predict unhealthy versus healthy is calculated.\n",
    "- There are no pre-existing benchmarks to beat. Sometimes when developing a new predictive model, it needs to beat an existing system.\n",
    "- The application will generate a static html page for the visualizations and raw csv files for the predictions.\n",
    "\n",
    "\n",
    "## General approach\n",
    "Now that there is a defined problem, here are the steps needed:\n",
    "1. Find a data source for pm10 values with at least one hour resolution for Australia.  \n",
    "2. Explore the data and perform some analysis on its properties.\n",
    "3. Perform feature engineering to transform the data into training and test data sets.\n",
    "4. Train a machine learning model with the training data.\n",
    "5. Infer predictions using the trained model on the test data.\n",
    "6. Calculate the categorical class for the predicitons based on the ranges above.\n",
    "7. Evaluate the trained model by comparing actuals versus predicted.\n",
    "8. Create graphs and CSV files of the predictions.\n",
    "9. Create a machine learning pipeline to automate all of the above.\n",
    "> **Agility Is Important:** Machine learning projects are not linear. Many of the steps above could require repeating previous steps. For example, the data sources might be missing data, and the the problem statement needs to be reworked. Also, ehe evaluated model might not meet evaluation criteria, so additional data needs to be found. An agile approach that allows for experimentation, failure and redoing steps is required. \n",
    "\n",
    "## Data Discovery \n",
    "The data used to train the forecasting models needs to be found first. There are many open data sets available, as well as data from your own organizations. One good source of data is [Registry of Open Data on AWS](https://registry.opendata.aws). The registry has a simple search interface that can be used to find data. Lets search for \"air quality\" data sets:\n",
    "\n",
    "![Registry of Open Data](img/rod_screen_shot.png)\n",
    "\n",
    "The [OpenAQ](https://registry.opendata.aws/openaq/) data set has per city air quality measurements at hourly frequency, and is a perfect fit for the problem.\n",
    "\n",
    "![Open AQ](img/openaq_screen_shot.png)\n",
    "\n",
    "The data is publicy available on S3 and contains many locations and different quality measurements. It needs to be filtered down to only Australia pm10 measurements. By Amazon Athena to query the data, a smaller csv file containing only what is needed can be created.\n",
    "\n",
    "### Import project dependencies\n",
    "Before beggining, first import all the python modules, configuration settings and helper functions needed. \n",
    "\n",
    "> **Note:** When developing your own projects, it's recommended to separate lower level code out to a python module to make the notebook more readable. This also makes putting the final code into production format easier as it is already partly modularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openaq.project_dependencies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create external Athena table\n",
    "The table used to query against is backed by a publicly readable S3 bucket provided by Open AQ. The data definition language (DDL) query sets the external tables S3 location, and table definition, so it can be queried by Athena. A simple function that uses the boto3 API's to execute operations with Athena and wait for results is used to both create the table and query it. \n",
    "\n",
    "> Note: The table definition is based on this [code](https://gist.github.com/jflasher/573525aff9a5d8a966e5718272ceb25a). The code creates an external table in Amazon Athena. To save the OpenAQ organization on data transport costs out of region, run Athena queries in the US East region if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Executing query:<br><br><code>CREATE EXTERNAL TABLE openaq(\n",
       "  `date` struct<utc:string,local:string>, \n",
       "  `parameter` string, \n",
       "  `location` string, \n",
       "  value float, \n",
       "  unit string, \n",
       "  city string, \n",
       "  attribution array<struct<name:string,url:string>>, \n",
       "  averagingperiod struct<unit:string,value:float>, \n",
       "  coordinates struct<latitude:float,longitude:float>, \n",
       "  country string, \n",
       "  sourcename string, \n",
       "  sourcetype string, \n",
       "  mobile string\n",
       ")\n",
       "ROW FORMAT SERDE  'org.openx.data.jsonserde.JsonSerDe' \n",
       "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' \n",
       "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
       "LOCATION 's3://openaq-fetches/realtime-gzipped'</code><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "query 10b7ca1e-66be-4ca2-90f4-69b0f932bcb7 failed with status FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d78adbbf367a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_results_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mathena_create_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'openaq/sql/openaq.ddl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/openaq/project_dependencies.py\u001b[0m in \u001b[0;36mathena_create_table\u001b[0;34m(query_file, wait)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mathena_create_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mcreate_table_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcreate_table_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mathena_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_table_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_table_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openaq/project_dependencies.py\u001b[0m in \u001b[0;36mathena_execute\u001b[0;34m(query_file, results_uri, ext, wait)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'query {query_id} failed with status {status}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: query 10b7ca1e-66be-4ca2-90f4-69b0f932bcb7 failed with status FAILED"
     ]
    }
   ],
   "source": [
    "create_results_uri = athena_create_table('openaq/sql/openaq.ddl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query data with Athena\n",
    "The data manipulation language (DML) query queries the external OpenAQ table for the data required, and places it into an S3 bucket owned by this account. \n",
    "\n",
    "> **Try this:** If you would like to modify this lab to be more relavent to where you live, try copying and modifying the sql in the `openaq/sql` file to get data for your country. Not all countries have data. If you are interested in building your own air quality measurment sensor, do a search on \"DIY air quality monitoring\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Executing query:<br><br><code>select\n",
       "    country,\n",
       "    city,\n",
       "    location,\n",
       "    parameter,\n",
       "    cast(from_iso8601_timestamp(date.utc) as timestamp) as timestamp,\n",
       "    value,\n",
       "    coordinates.latitude as point_latitude,\n",
       "    coordinates.longitude as point_longitude\n",
       "from openaq\n",
       "where \n",
       "    country = 'AU'  \n",
       "    and parameter = 'pm10' \n",
       "    and strpos(city, 'Sydney') != 0</code><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "results are located at <a href=\"https://s3.console.aws.amazon.com/s3/object/231935085477-openaq-lab/athena/results/5a1de731-ee24-4e79-be5b-8d399c530ad0.csv?region=us-east-1&tab=overview\">s3://231935085477-openaq-lab/athena/results/5a1de731-ee24-4e79-be5b-8d399c530ad0.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_results_uri = athena_query_table('openaq/sql/sydney.dml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exercise: Copy past the SQL statement above. Now go to the [Athena console](https://console.aws.amazon.com/athena/home), paste the SQL in the query window and modify it to get the average (use the \"avg\" function) value of the Carbon Monoxide (code \"co\") measurements for Melbourne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Feature engineering is the process of transforming raw data into features that can be used for training and testing a machine learning model. Time series data can be stored in unsorted format, can have missing values or may be captured at a higher or lower frquency than what is needed. In addition, machine learning algorithms require the data be put in a standard format for both training and inference. The main steps taken to format the raw data are outlined here.\n",
    "\n",
    "![Feature Engineering](img/feature_engineering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read raw data\n",
    "The Athena query that was previously run places the query results in CSV format in an S3 bucket. The path to the csv object is a combination of a base path defined for all Athena query results and the query id. Using these values, we constuct an S3 path, and then read the data into an in memory panda data frame. Pandas is a data science library that makes it easy to process and explore time series data. Using the pandas csv function the query results can be read directly into an in memory data frame. \n",
    "\n",
    "> **Explore:** For more information on working with Pandas, check out the  [user guide](https://pandas.pydata.org/docs/user_guide/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading s3://231935085477-openaq-lab/athena/results/5a1de731-ee24-4e79-be5b-8d399c530ad0.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>location</th>\n",
       "      <th>parameter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>point_latitude</th>\n",
       "      <th>point_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AU</td>\n",
       "      <td>Sydney East</td>\n",
       "      <td>Randwick</td>\n",
       "      <td>pm10</td>\n",
       "      <td>2017-08-13 20:00:00</td>\n",
       "      <td>13.7</td>\n",
       "      <td>-33.933334</td>\n",
       "      <td>151.24194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AU</td>\n",
       "      <td>Sydney East</td>\n",
       "      <td>Rozelle</td>\n",
       "      <td>pm10</td>\n",
       "      <td>2017-08-13 20:00:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>-33.865833</td>\n",
       "      <td>151.16250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AU</td>\n",
       "      <td>Sydney East</td>\n",
       "      <td>Chullora</td>\n",
       "      <td>pm10</td>\n",
       "      <td>2017-08-13 20:00:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>-33.893890</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AU</td>\n",
       "      <td>Sydney East</td>\n",
       "      <td>Earlwood</td>\n",
       "      <td>pm10</td>\n",
       "      <td>2017-08-13 20:00:00</td>\n",
       "      <td>9.3</td>\n",
       "      <td>-33.917778</td>\n",
       "      <td>151.13472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AU</td>\n",
       "      <td>Sydney North-west</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>pm10</td>\n",
       "      <td>2017-08-13 20:00:00</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-33.618332</td>\n",
       "      <td>150.74583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country               city  location parameter           timestamp  value  \\\n",
       "0      AU        Sydney East  Randwick      pm10 2017-08-13 20:00:00   13.7   \n",
       "1      AU        Sydney East   Rozelle      pm10 2017-08-13 20:00:00    8.6   \n",
       "2      AU        Sydney East  Chullora      pm10 2017-08-13 20:00:00    8.4   \n",
       "3      AU        Sydney East  Earlwood      pm10 2017-08-13 20:00:00    9.3   \n",
       "4      AU  Sydney North-west  Richmond      pm10 2017-08-13 20:00:00   16.0   \n",
       "\n",
       "   point_latitude  point_longitude  \n",
       "0      -33.933334        151.24194  \n",
       "1      -33.865833        151.16250  \n",
       "2      -33.893890        151.04527  \n",
       "3      -33.917778        151.13472  \n",
       "4      -33.618332        150.74583  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (f'reading {query_results_uri}')\n",
    "raw = pd.read_csv(query_results_uri, parse_dates=['timestamp'])\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Sort and index by location and time \n",
    "Before doing any transformations the raw data needs to be organized by time and locations. In pandas, we can easily sort by data frame columns, and then create an index for all the categorical columns as well as time. The names of index columns are refered to as \"levels\" in pandas. After executing the query below, you will notice that the readings are now sorted and grouped by the index levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>point_latitude</th>\n",
       "      <th>point_longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>location</th>\n",
       "      <th>parameter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AU</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Sydney East</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Chullora</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">pm10</th>\n",
       "      <th>2016-04-09 20:00:00</th>\n",
       "      <td>12.2</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09 21:00:00</th>\n",
       "      <td>12.3</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09 22:00:00</th>\n",
       "      <td>12.4</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09 23:00:00</th>\n",
       "      <td>12.5</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-10 00:00:00</th>\n",
       "      <td>12.8</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            value  \\\n",
       "country city        location parameter timestamp                    \n",
       "AU      Sydney East Chullora pm10      2016-04-09 20:00:00   12.2   \n",
       "                                       2016-04-09 21:00:00   12.3   \n",
       "                                       2016-04-09 22:00:00   12.4   \n",
       "                                       2016-04-09 23:00:00   12.5   \n",
       "                                       2016-04-10 00:00:00   12.8   \n",
       "\n",
       "                                                            point_latitude  \\\n",
       "country city        location parameter timestamp                             \n",
       "AU      Sydney East Chullora pm10      2016-04-09 20:00:00       -33.89389   \n",
       "                                       2016-04-09 21:00:00       -33.89389   \n",
       "                                       2016-04-09 22:00:00       -33.89389   \n",
       "                                       2016-04-09 23:00:00       -33.89389   \n",
       "                                       2016-04-10 00:00:00       -33.89389   \n",
       "\n",
       "                                                            point_longitude  \n",
       "country city        location parameter timestamp                             \n",
       "AU      Sydney East Chullora pm10      2016-04-09 20:00:00        151.04527  \n",
       "                                       2016-04-09 21:00:00        151.04527  \n",
       "                                       2016-04-09 22:00:00        151.04527  \n",
       "                                       2016-04-09 23:00:00        151.04527  \n",
       "                                       2016-04-10 00:00:00        151.04527  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_levels = ['country', 'city', 'location', 'parameter']\n",
    "index_levels = categorical_levels + ['timestamp']\n",
    "indexed = raw.sort_values(index_levels, ascending=True)\n",
    "indexed = indexed.set_index(index_levels)\n",
    "indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Get the mean, minimum or maximum pm10 values for all of the selected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Downsample to hourly samples by maximum value\n",
    "Downsampling combines multiple samples that may fall into the same window of time according to the sampling frequency. Some air quality instruments may measure values more than once and hour. Since we want to predict the peak value in any given hour, we will take the maximum of the values for each hour. Depending on the time series use case, you could also use the mean, first, last and minimum value for downsampling. \n",
    "\n",
    "> **Note**: Using max on lat/long columns is ok as long as all entries in a single time series have the same lat/long values, which is the case for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>point_latitude</th>\n",
       "      <th>point_longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>location</th>\n",
       "      <th>parameter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AU</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Sydney East</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Chullora</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">pm10</th>\n",
       "      <th>2016-04-09 20:00:00</th>\n",
       "      <td>12.2</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09 21:00:00</th>\n",
       "      <td>12.3</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09 22:00:00</th>\n",
       "      <td>12.4</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09 23:00:00</th>\n",
       "      <td>12.5</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-10 00:00:00</th>\n",
       "      <td>12.8</td>\n",
       "      <td>-33.89389</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            value  \\\n",
       "country city        location parameter timestamp                    \n",
       "AU      Sydney East Chullora pm10      2016-04-09 20:00:00   12.2   \n",
       "                                       2016-04-09 21:00:00   12.3   \n",
       "                                       2016-04-09 22:00:00   12.4   \n",
       "                                       2016-04-09 23:00:00   12.5   \n",
       "                                       2016-04-10 00:00:00   12.8   \n",
       "\n",
       "                                                            point_latitude  \\\n",
       "country city        location parameter timestamp                             \n",
       "AU      Sydney East Chullora pm10      2016-04-09 20:00:00       -33.89389   \n",
       "                                       2016-04-09 21:00:00       -33.89389   \n",
       "                                       2016-04-09 22:00:00       -33.89389   \n",
       "                                       2016-04-09 23:00:00       -33.89389   \n",
       "                                       2016-04-10 00:00:00       -33.89389   \n",
       "\n",
       "                                                            point_longitude  \n",
       "country city        location parameter timestamp                             \n",
       "AU      Sydney East Chullora pm10      2016-04-09 20:00:00        151.04527  \n",
       "                                       2016-04-09 21:00:00        151.04527  \n",
       "                                       2016-04-09 22:00:00        151.04527  \n",
       "                                       2016-04-09 23:00:00        151.04527  \n",
       "                                       2016-04-10 00:00:00        151.04527  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled = indexed.groupby(categorical_levels + [pd.Grouper(level='timestamp', freq='1H')]).max()\n",
    "downsampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Copy the expression above and modify it to downsample with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Back fill missing values\n",
    "A common problem with time series data is that values are often missing. It's a important to determine how many missing values there are in your data and perform filling. \n",
    "\n",
    "In order to fill missing values there are two steps. First we re-index the data for the desired frequency of 1 hour. This will create a NaN (not a number) entry for any missing values. Once we have the reindexed data we can then calculate some statistics on the number of NaN values. \n",
    "\n",
    "To check for missing values, we filter all non-null columns, group them by location, count them per group, and then descibe the summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>point_latitude</th>\n",
       "      <th>point_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value  point_latitude  point_longitude\n",
       "count   18.0            18.0             18.0\n",
       "mean     0.0             0.0              0.0\n",
       "std      0.0             0.0              0.0\n",
       "min      0.0             0.0              0.0\n",
       "25%      0.0             0.0              0.0\n",
       "50%      0.0             0.0              0.0\n",
       "75%      0.0             0.0              0.0\n",
       "max      0.0             0.0              0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_missing_hours(df):\n",
    "    df = df.reset_index(level=categorical_levels, drop=True)                                    \n",
    "    index = pd.date_range(df.index.min(), df.index.max(), freq='1H')\n",
    "    return df.reindex(pd.Index(index, name='timestamp'))\n",
    "\n",
    "filled = downsampled.groupby(level=categorical_levels).apply(fill_missing_hours)\n",
    "filled[filled['value'].isnull()].groupby('location').count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, are not may not be any missing values. Depending on the country the data was gathered from, there may be more. If there are missing values, the next step is to replace the NaN values with something else. For this case, we will linearly interpolate between the last know values for the pm10 measurement. By also rounding to two decimal places, all of the interpolated values will match the actual values precision. We will also fill any missing latitudes or longitudes with the first value available for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>point_latitude</th>\n",
       "      <th>point_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value  point_latitude  point_longitude\n",
       "count    0.0             0.0              0.0\n",
       "mean     NaN             NaN              NaN\n",
       "std      NaN             NaN              NaN\n",
       "min      NaN             NaN              NaN\n",
       "25%      NaN             NaN              NaN\n",
       "50%      NaN             NaN              NaN\n",
       "75%      NaN             NaN              NaN\n",
       "max      NaN             NaN              NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled['value'] = filled['value'].interpolate().round(2)\n",
    "filled['point_latitude'] = filled['point_latitude'].fillna(method='pad')\n",
    "filled['point_longitude'] = filled['point_longitude'].fillna(method='pad')\n",
    "\n",
    "filled[filled['value'].isnull()].groupby('location').count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create features\n",
    "Now that we have a contiguous data set at the proper frequency, we can transform the data frame into a format that is required for training and testing. The DeepAR algorithm we will be using to train the machine learning model, requires data in the following format. \n",
    "\n",
    "`{start: <start time of series>, target: [v1,v2,v3....], cat: [id1, id2, id3...]}`\n",
    "\n",
    "For each location, the features are the start time for the time series, a contiguous list of all values, and an optional categorical array. This categorical array is composed of all the id's for country, city, location and measurment type.\n",
    "\n",
    "> **Explore:** Read more about the [DeepAR feature format](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar-inputoutput).\n",
    "\n",
    "#### Aggregate time series into rows\n",
    "To get to the required format, a single row per time series needs to be created. This is done by aggregating each locations time series values into a single list per row, and by only retainng the timestamp of the first value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = filled.reset_index(level=4)\\\n",
    "    .groupby(level=categorical_levels)\\\n",
    "    .agg(dict(timestamp='first', value=list, point_latitude='first', point_longitude='first'))\\\n",
    "    .rename(columns=dict(timestamp='start', value='target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to relate each time series to future predictions generated from the trained model, a unique id per time series is needed. Once there is a unique id, we can break the data into a metadata dataframe and a features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>point_latitude</th>\n",
       "      <th>point_longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>location</th>\n",
       "      <th>parameter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Chullora</th>\n",
       "      <th>pm10</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....</td>\n",
       "      <td>-33.893890</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Cook And Phillip</th>\n",
       "      <th>pm10</th>\n",
       "      <td>2019-09-08 21:00:00</td>\n",
       "      <td>[5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...</td>\n",
       "      <td>-33.893890</td>\n",
       "      <td>151.04527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Earlwood</th>\n",
       "      <th>pm10</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....</td>\n",
       "      <td>-33.917778</td>\n",
       "      <td>151.13472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Macquarie Park</th>\n",
       "      <th>pm10</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>[5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...</td>\n",
       "      <td>-33.917778</td>\n",
       "      <td>151.13472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Randwick</th>\n",
       "      <th>pm10</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...</td>\n",
       "      <td>-33.933334</td>\n",
       "      <td>151.24194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                start  \\\n",
       "id country city        location         parameter                       \n",
       "0  AU      Sydney East Chullora         pm10      2016-04-09 20:00:00   \n",
       "1  AU      Sydney East Cook And Phillip pm10      2019-09-08 21:00:00   \n",
       "2  AU      Sydney East Earlwood         pm10      2016-04-09 20:00:00   \n",
       "3  AU      Sydney East Macquarie Park   pm10      2017-08-31 00:00:00   \n",
       "4  AU      Sydney East Randwick         pm10      2016-04-09 20:00:00   \n",
       "\n",
       "                                                                                              target  \\\n",
       "id country city        location         parameter                                                      \n",
       "0  AU      Sydney East Chullora         pm10       [12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....   \n",
       "1  AU      Sydney East Cook And Phillip pm10       [5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...   \n",
       "2  AU      Sydney East Earlwood         pm10       [11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....   \n",
       "3  AU      Sydney East Macquarie Park   pm10       [5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...   \n",
       "4  AU      Sydney East Randwick         pm10       [20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...   \n",
       "\n",
       "                                                   point_latitude  \\\n",
       "id country city        location         parameter                   \n",
       "0  AU      Sydney East Chullora         pm10           -33.893890   \n",
       "1  AU      Sydney East Cook And Phillip pm10           -33.893890   \n",
       "2  AU      Sydney East Earlwood         pm10           -33.917778   \n",
       "3  AU      Sydney East Macquarie Park   pm10           -33.917778   \n",
       "4  AU      Sydney East Randwick         pm10           -33.933334   \n",
       "\n",
       "                                                   point_longitude  \n",
       "id country city        location         parameter                   \n",
       "0  AU      Sydney East Chullora         pm10             151.04527  \n",
       "1  AU      Sydney East Cook And Phillip pm10             151.04527  \n",
       "2  AU      Sydney East Earlwood         pm10             151.13472  \n",
       "3  AU      Sydney East Macquarie Park   pm10             151.13472  \n",
       "4  AU      Sydney East Randwick         pm10             151.24194  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated['id'] = np.arange(len(aggregated))\n",
    "aggregated.reset_index(inplace=True)\n",
    "aggregated.set_index(['id']+categorical_levels, inplace=True)\n",
    "aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform lat/long to geometry\n",
    "Since our time series also have a geographical location, we will also keep track of lat/long so we can use it for displaying our results on maps. GeoPandas enables geo searches within pandas data frames, and requires a geometry object which is created by combining the latitude and longitude columns into a single point geometry column. The code below does all of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>location</th>\n",
       "      <th>parameter</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Chullora</th>\n",
       "      <th>pm10</th>\n",
       "      <td>POINT (151.04527 -33.89389)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Cook And Phillip</th>\n",
       "      <th>pm10</th>\n",
       "      <td>POINT (151.04527 -33.89389)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Earlwood</th>\n",
       "      <th>pm10</th>\n",
       "      <td>POINT (151.13472 -33.91778)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Macquarie Park</th>\n",
       "      <th>pm10</th>\n",
       "      <td>POINT (151.13472 -33.91778)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>AU</th>\n",
       "      <th>Sydney East</th>\n",
       "      <th>Randwick</th>\n",
       "      <th>pm10</th>\n",
       "      <td>POINT (151.24194 -33.93333)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      geometry\n",
       "id country city        location         parameter                             \n",
       "0  AU      Sydney East Chullora         pm10       POINT (151.04527 -33.89389)\n",
       "1  AU      Sydney East Cook And Phillip pm10       POINT (151.04527 -33.89389)\n",
       "2  AU      Sydney East Earlwood         pm10       POINT (151.13472 -33.91778)\n",
       "3  AU      Sydney East Macquarie Park   pm10       POINT (151.13472 -33.91778)\n",
       "4  AU      Sydney East Randwick         pm10       POINT (151.24194 -33.93333)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = gpd.GeoDataFrame(\n",
    "    aggregated.drop(columns=['target','start']), \n",
    "    geometry=gpd.points_from_xy(aggregated.point_longitude, aggregated.point_latitude), \n",
    "    crs={\"init\":\"EPSG:4326\"}\n",
    ")\n",
    "metadata.drop(columns=['point_longitude', 'point_latitude'], inplace=True)\n",
    "\n",
    "# missing lat/longs\n",
    "#metadata.loc[pd.IndexSlice[:,'AU','South West Queensland','Miles Airport',:], 'geometry'] = Point(150.165, -26.809167) \n",
    "#metadata.loc[pd.IndexSlice[:,'AU','South West Queensland','Hopeland',:], 'geometry'] = Point(150.6655, -26.8841)\n",
    "\n",
    "# set geometry index\n",
    "metadata.set_geometry('geometry')\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a map plot\n",
    "The map plot will display a map that contains all the locations for our air quality measurments. We are using the bokeh library to plot the map. We will use this map later to select which locations to view predictions for.\n",
    "\n",
    "> **Explore:** For more information on plotting with bokeh, check out the [user guide](https://docs.bokeh.org/en/latest/index.html). Other popular plotting libraries include [matplotlib](https://matplotlib.org/) and [plotly](https://plotly.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"cd11c144-1904-4866-a420-e213e4a25db3\" data-root-id=\"1004\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"6c3c3d69-c7af-44d3-b3bb-6a310a833ec2\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1015\"}],\"center\":[{\"id\":\"1022\"},{\"id\":\"1030\"}],\"left\":[{\"id\":\"1023\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1037\"},{\"id\":\"1042\"}],\"title\":{\"id\":\"1005\"},\"toolbar\":{\"id\":\"1034\"},\"x_range\":{\"id\":\"1007\"},\"x_scale\":{\"id\":\"1011\"},\"y_range\":{\"id\":\"1009\"},\"y_scale\":{\"id\":\"1013\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"geojson\":\"{\\\"type\\\": \\\"FeatureCollection\\\", \\\"features\\\": [{\\\"id\\\": \\\"0\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney East\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 0, \\\"location\\\": \\\"Chullora\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16814282.54313252, -4014562.929164007]}}, {\\\"id\\\": \\\"1\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney East\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 1, \\\"location\\\": \\\"Cook And Phillip\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16814282.54313252, -4014562.929164007]}}, {\\\"id\\\": \\\"2\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney East\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 2, \\\"location\\\": \\\"Earlwood\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16824240.07158398, -4017766.956446742]}}, {\\\"id\\\": \\\"3\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney East\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 3, \\\"location\\\": \\\"Macquarie Park\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16824240.07158398, -4017766.956446742]}}, {\\\"id\\\": \\\"4\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney East\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 4, \\\"location\\\": \\\"Randwick\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16836175.74738683, -4019853.9199722246]}}, {\\\"id\\\": \\\"5\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney East\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 5, \\\"location\\\": \\\"Rozelle\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16827332.527038217, -4010800.871956737]}}, {\\\"id\\\": \\\"6\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney North-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 6, \\\"location\\\": \\\"North Parramatta\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16827332.527038217, -4010800.871956737]}}, {\\\"id\\\": \\\"7\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney North-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 7, \\\"location\\\": \\\"Parramatta North\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16827332.527038217, -4010800.871956737]}}, {\\\"id\\\": \\\"8\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney North-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 8, \\\"location\\\": \\\"Prospect\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16799502.654339895, -4001271.667228511]}}, {\\\"id\\\": \\\"9\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney North-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 9, \\\"location\\\": \\\"Richmond\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16780949.034809384, -3977667.7520220666]}}, {\\\"id\\\": \\\"10\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney North-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 10, \\\"location\\\": \\\"Rouse Hill\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16780949.034809384, -3977667.7520220666]}}, {\\\"id\\\": \\\"11\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney North-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 11, \\\"location\\\": \\\"St Marys\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16783176.537820157, -4001606.286003931]}}, {\\\"id\\\": \\\"12\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney South-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 12, \\\"location\\\": \\\"Bargo\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16762488.923651136, -4070166.8778622802]}}, {\\\"id\\\": \\\"13\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney South-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 13, \\\"location\\\": \\\"Bringelly\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16782649.996628705, -4017990.5797820785]}}, {\\\"id\\\": \\\"14\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney South-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 14, \\\"location\\\": \\\"Camden\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16774765.237095816, -4034398.3936987137]}}, {\\\"id\\\": \\\"15\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney South-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 15, \\\"location\\\": \\\"Campbelltown West\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16786452.6704342, -4037757.1576953162]}}, {\\\"id\\\": \\\"16\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney South-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 16, \\\"location\\\": \\\"Liverpool\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16798761.266531214, -4019779.1873265067]}}, {\\\"id\\\": \\\"17\\\", \\\"type\\\": \\\"Feature\\\", \\\"properties\\\": {\\\"city\\\": \\\"Sydney South-west\\\", \\\"country\\\": \\\"AU\\\", \\\"id\\\": 17, \\\"location\\\": \\\"Oakdale\\\", \\\"parameter\\\": \\\"pm10\\\"}, \\\"geometry\\\": {\\\"type\\\": \\\"Point\\\", \\\"coordinates\\\": [16753273.896203266, -4035928.243096953]}}]}\",\"selected\":{\"id\":\"1051\"},\"selection_policy\":{\"id\":\"1050\"}},\"id\":\"1003\",\"type\":\"GeoJSONDataSource\"},{\"attributes\":{\"overlay\":{\"id\":\"1032\"}},\"id\":\"1031\",\"type\":\"LassoSelectTool\"},{\"attributes\":{\"source\":{\"id\":\"1003\"}},\"id\":\"1043\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Country\",\"@country\"],[\"City\",\"@city\"],[\"Location\",\"@location\"]]},\"id\":\"1033\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"Selection\"},{\"attributes\":{\"axis\":{\"id\":\"1023\"},\"dimension\":1,\"ticker\":null},\"id\":\"1030\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis_label_standoff\":10,\"axis_label_text_color\":\"#5B5B5B\",\"axis_label_text_font\":\"Calibri Light\",\"axis_label_text_font_size\":\"1.15em\",\"axis_label_text_font_style\":\"bold\",\"axis_line_alpha\":1,\"axis_line_color\":\"#5B5B5B\",\"formatter\":{\"id\":\"1018\"},\"major_label_text_color\":\"#5B5B5B\",\"major_label_text_font\":\"Calibri Light\",\"major_label_text_font_size\":\"0.95em\",\"major_label_text_font_style\":\"bold\",\"major_tick_in\":0,\"major_tick_line_alpha\":0.25,\"major_tick_line_color\":\"#5B5B5B\",\"major_tick_out\":3,\"minor_tick_line_alpha\":0.25,\"minor_tick_line_color\":\"#5B5B5B\",\"ticker\":{\"id\":\"1016\"}},\"id\":\"1015\",\"type\":\"MercatorAxis\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1026\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.8},\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":5},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1040\",\"type\":\"Circle\"},{\"attributes\":{\"tile_source\":{\"id\":\"1002\"}},\"id\":\"1037\",\"type\":\"TileRenderer\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis_label_standoff\":10,\"axis_label_text_color\":\"#5B5B5B\",\"axis_label_text_font\":\"Calibri Light\",\"axis_label_text_font_size\":\"1.15em\",\"axis_label_text_font_style\":\"bold\",\"axis_line_alpha\":1,\"axis_line_color\":\"#5B5B5B\",\"formatter\":{\"id\":\"1026\"},\"major_label_text_color\":\"#5B5B5B\",\"major_label_text_font\":\"Calibri Light\",\"major_label_text_font_size\":\"0.95em\",\"major_label_text_font_style\":\"bold\",\"major_tick_in\":0,\"major_tick_line_alpha\":0.25,\"major_tick_line_color\":\"#5B5B5B\",\"major_tick_out\":3,\"minor_tick_line_alpha\":0.25,\"minor_tick_line_color\":\"#5B5B5B\",\"ticker\":{\"id\":\"1024\"}},\"id\":\"1023\",\"type\":\"MercatorAxis\"},{\"attributes\":{\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"xs_units\":\"screen\",\"ys_units\":\"screen\"},\"id\":\"1032\",\"type\":\"PolyAnnotation\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"blue\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":5},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1041\",\"type\":\"Circle\"},{\"attributes\":{\"axis\":{\"id\":\"1015\"},\"ticker\":null},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{\"attribution\":\"&copy; <a href=\\\"https://www.openstreetmap.org/copyright\\\">OpenStreetMap</a> contributors,&copy; <a href=\\\"https://cartodb.com/attributions\\\">CartoDB</a>\",\"url\":\"https://tiles.basemaps.cartocdn.com/light_all/{z}/{x}/{y}.png\"},\"id\":\"1002\",\"type\":\"WMTSTileSource\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1024\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1018\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1031\"},{\"id\":\"1033\"}]},\"id\":\"1034\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"text\":\"Measurement Locations\",\"text_color\":{\"value\":\"#5B5B5B\"},\"text_font\":\"Calibri Light\",\"text_font_size\":{\"value\":\"1.25em\"},\"text_font_style\":\"bold\"},\"id\":\"1005\",\"type\":\"Title\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1016\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"1003\"},\"glyph\":{\"id\":\"1040\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1041\"},\"selection_glyph\":null,\"view\":{\"id\":\"1043\"}},\"id\":\"1042\",\"type\":\"GlyphRenderer\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"6c3c3d69-c7af-44d3-b3bb-6a310a833ec2\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"cd11c144-1904-4866-a420-e213e4a25db3\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "tile_provider = get_provider(CARTODBPOSITRON)\n",
    "curdoc().theme = 'light_minimal'\n",
    "\n",
    "map_df = metadata.to_crs(\"EPSG:3857\").reset_index()\n",
    "map_source = GeoJSONDataSource(geojson=map_df.to_json(na='drop'))\n",
    "\n",
    "tooltips = [\n",
    "    ('Country', '@country'),\n",
    "    ('City', '@city'),\n",
    "    ('Location', '@location')\n",
    "]\n",
    "\n",
    "map_plot = figure(\n",
    "    title=\"Measurement Locations\", \n",
    "    plot_width=800, plot_height=400, \n",
    "    x_axis_type=\"mercator\", y_axis_type=\"mercator\", \n",
    "    tooltips=tooltips, \n",
    "    tools=\"lasso_select\"\n",
    ")\n",
    "\n",
    "map_plot.add_tile(tile_provider)\n",
    "map_circles = map_plot.circle(x=\"x\", y=\"y\", size=5, fill_color=\"blue\", fill_alpha=0.8, source=map_source)\n",
    "show(map_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Try to change the plot size, color of the location, and add 'parameter' as a tooltip to the above plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add categorical feature\n",
    "The DeepAR model allows time series to be associated by categorical features. This is a key features, as we are building a single machine leanring model to predict air quality for many different locations. The categorical features enable the deep learning model to build an internal representation of how locations are related to each other. To do this, we create a list of ids that represent each of categorical features. The country, city, location, and measuremnt type will each be codified as an id, and combined into a list for each locations time series.\n",
    "\n",
    "> **Note:** Since we only are building a model for pm10 values in Australia, only the city and location ids will have multiple values. We could modify the initial query to get multiple measurement types across many countries and train a more comprehensive model using exact the same steps we have walked through.\n",
    "\n",
    "To get a set of categorical ids, we use the pandas factorize method to generate id's for categorical values. We then apply string to categorical conversion across every row of our aggregated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-08 21:00:00</td>\n",
       "      <td>[5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>[5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start                                             target  \\\n",
       "id                                                                          \n",
       "0  2016-04-09 20:00:00  [12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....   \n",
       "1  2019-09-08 21:00:00  [5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...   \n",
       "2  2016-04-09 20:00:00  [11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....   \n",
       "3  2017-08-31 00:00:00  [5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...   \n",
       "4  2016-04-09 20:00:00  [20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...   \n",
       "\n",
       "             cat  \n",
       "id                \n",
       "0   [0, 0, 0, 0]  \n",
       "1   [0, 0, 1, 0]  \n",
       "2   [0, 0, 2, 0]  \n",
       "3   [0, 0, 3, 0]  \n",
       "4   [0, 0, 4, 0]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_ids = [level+'_id' for level in categorical_levels]\n",
    "for l in level_ids:\n",
    "    aggregated[l], index = pd.factorize(aggregated.index.get_level_values(l[:-3]))\n",
    "    \n",
    "aggregated['cat'] = aggregated.apply(lambda columns: [columns[l] for l in level_ids], axis=1)\n",
    "features = aggregated.drop(columns=level_ids+ ['point_longitude', 'point_latitude'])\n",
    "features.reset_index(level=categorical_levels, inplace=True, drop=True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Split features into training and a test sets\n",
    "In order to evaluate the final model, the features need to be split into training and test sets. To accurately get an idea of how the model will perform on previously unseen data, time series data should be split according to a cutoff date.\n",
    "\n",
    "![training and test split by time](img/train_test_split.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-08 21:00:00</td>\n",
       "      <td>[5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>[5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start                                             target  \\\n",
       "id                                                                          \n",
       "0  2016-04-09 20:00:00  [12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....   \n",
       "1  2019-09-08 21:00:00  [5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...   \n",
       "2  2016-04-09 20:00:00  [11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....   \n",
       "3  2017-08-31 00:00:00  [5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...   \n",
       "4  2016-04-09 20:00:00  [20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...   \n",
       "\n",
       "             cat  \n",
       "id                \n",
       "0   [0, 0, 0, 0]  \n",
       "1   [0, 0, 1, 0]  \n",
       "2   [0, 0, 2, 0]  \n",
       "3   [0, 0, 3, 0]  \n",
       "4   [0, 0, 4, 0]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split_date = date.today() - timedelta(days=30)\n",
    "train = filter_dates(features, None, train_test_split_date, '1H')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[22.64, 21.95, 21.27, 20.59, 19.91, 19.22, 18....</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[7.12, 7.24, 7.35, 7.46, 7.57, 7.68, 7.79, 7.9...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[6.35, 6.63, 6.91, 7.18, 7.46, 7.74, 8.01, 8.2...</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[6.96, 7.09, 7.22, 7.35, 7.48, 7.61, 7.74, 7.8...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[12.54, 12.45, 12.37, 12.29, 12.21, 12.12, 12....</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start                                             target           cat\n",
       "id                                                                            \n",
       "0  2020-07-20  [22.64, 21.95, 21.27, 20.59, 19.91, 19.22, 18....  [0, 0, 0, 0]\n",
       "1  2020-07-20  [7.12, 7.24, 7.35, 7.46, 7.57, 7.68, 7.79, 7.9...  [0, 0, 1, 0]\n",
       "2  2020-07-20  [6.35, 6.63, 6.91, 7.18, 7.46, 7.74, 8.01, 8.2...  [0, 0, 2, 0]\n",
       "3  2020-07-20  [6.96, 7.09, 7.22, 7.35, 7.48, 7.61, 7.74, 7.8...  [0, 0, 3, 0]\n",
       "4  2020-07-20  [12.54, 12.45, 12.37, 12.29, 12.21, 12.12, 12....  [0, 0, 4, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = filter_dates(features, train_test_split_date, None, '1H')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DeepAR model\n",
    "Now that we have our features split into to train and test sets, we can train a machine learnign model. For this problem, we will use DeepAR, an first party algorythm available in SageMaker. DeepAR is a deep leanring algorithm that creates a single machine learning model for multiple related time series.\n",
    "\n",
    "### Create estimator\n",
    "To create an estimator, get the image name for the desired algorithm, the execution role and determin the number of training instances and type needed. For DeepAR a gpu accelerated instance is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker.estimator:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "image_uri = sagemaker.amazon.amazon_estimator.get_image_uri(region, 'forecasting-deepar', 'latest')\n",
    "output_path = f's3://{bucket_name}/sagemaker/output'\n",
    "\n",
    "training_job_name=None\n",
    "if training_job_name:\n",
    "    estimator = sagemaker.estimator.Estimator.attach(\n",
    "        training_job_name, \n",
    "        sagemaker_session= session\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        sagemaker_session= session,\n",
    "        image_name= image_uri,\n",
    "        role= sagemaker.get_execution_role() ,\n",
    "        train_instance_count= 1,\n",
    "        train_instance_type='ml.p2.xlarge',\n",
    "        base_job_name= 'deepar-openaq-demo',\n",
    "        output_path= output_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training data to S3\n",
    "In order to use the training data, we need to get it from in memory pandas data frames to a location in S3 where SageMaker can use it. We first write all of the train and test set features to a local location. We can then use the SageMaker upload_data method to upload them to S3, and get a reference id that is used later by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json('openaq/data/train.json', orient='records', lines=True)\n",
    "test.to_json('openaq/data/test.json', orient='records', lines=True) \n",
    "\n",
    "data = dict(\n",
    "    train= session.upload_data(path='openaq/data/train.json'),\n",
    "    test=  session.upload_data(path='openaq/data/test.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a hyperparameter optimization (HPO) job\n",
    "Machine learning models have tunable parameters that need to be set prior to training a model. These parameters affect both the time it takes to train a model and the quality metrics of the trained model. Hyperparameter optimization jobs train multiple models across ranges of these parameters to find the best combinations for the given data set. SageMaker HPO uses Bayesian optimization to rapidly find the optimal parameters much faster then a random or grid search. There are two sets of parameters below. The parameters with a range of values will be optimized, and static parameters won't be. For more information on the DeepAR hyper-parameters, visit the [developer guide.](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)\n",
    "\n",
    "![hyper-parameter tuning](img/hpo.png)\n",
    "\n",
    "> Note: HPO does not need to run during every training cycle. Because it is training multiple models, it is time consuming and expensive. Typically, you run HPO once, and then use static hyperparameters until the the ML models are no longer meeting evaluation metric goals. Since I have already run a large HPO job to determine the hyperparameters, they are set statically below, and we will only start the HPO job to demostrate how it operates. If you are training with a different country or measurement type, re-running a full HPO job will give you improved results.\n",
    "\n",
    "#### Set static hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static parameters are the ones we know to be the best based on previously run HPO jobs, as well as the non-tunable parameters like prediction length and time frequency that are set according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    time_freq= '1H'\n",
    "    ,early_stopping_patience= 40\n",
    "    ,prediction_length= 48\n",
    "    ,num_eval_samples= 10\n",
    "    ,test_quantiles= [0.5, 0.7, 0.9]\n",
    "    \n",
    "    # not setting these since HPO will use range of values\n",
    "    #,epochs= 400\n",
    "    #,context_length= 3\n",
    "    #,num_cells= 157\n",
    "    #,num_layers= 4\n",
    "    #,dropout_rate= 0.04\n",
    "    #,embedding_dimension= 12\n",
    "    #,mini_batch_size= 633\n",
    "    #,learning_rate= 0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyper-parameter ranges\n",
    "The hyperparameter ranges define the parameters we want the tuner to search across.\n",
    "\n",
    "> Explore: Look in the [user guide](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html) for DeepAR and add the recommended ranges for `embedding_dimension` to the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_ranges = dict(\n",
    "    epochs= IntegerParameter(1, 1000)\n",
    "    ,context_length= IntegerParameter(7, 48)\n",
    "    ,num_cells= IntegerParameter(30,200)\n",
    "    ,num_layers= IntegerParameter(1,8)\n",
    "    ,dropout_rate= ContinuousParameter(0.0, 0.2)\n",
    "    ,embedding_dimension= IntegerParameter(1, 50)\n",
    "    ,mini_batch_size= IntegerParameter(32, 1028)\n",
    "    ,learning_rate= ContinuousParameter(.00001, .1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and start the HPO job\n",
    "Once we have the HPO tuner defined, the fit method is called. This will trigger the launching of the training instances. When creating the tuner, an objective metrice is needed to compare each model it trains. For this model, the tuner will find the model with the minimal final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tuner:_TuningJob.start_new!!!\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: forecasting-deepar-200819-1304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br>The hyperparameter tuning job \"forecasting-deepar-200819-1304\" is now running. \n",
       "        To view it in the console click \n",
       "        <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/hyper-tuning-jobs\">here</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator.set_hyperparameters(**hpo)\n",
    "\n",
    "hpo_tuner = HyperparameterTuner(\n",
    "    estimator= estimator, \n",
    "    objective_metric_name= 'train:final_loss',\n",
    "    objective_type='Minimize',\n",
    "    hyperparameter_ranges= hpo_ranges,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=1\n",
    ")\n",
    "\n",
    "hpo_tuner.fit(data)\n",
    "hpo_job_name = hpo_tuner.latest_tuning_job.name\n",
    "\n",
    "display(HTML(f'''<br>The hyperparameter tuning job \"{hpo_job_name}\" is now running. \n",
    "        To view it in the console click \n",
    "        <a href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/hyper-tuning-jobs\">here</a>.\n",
    "    '''))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning job will run for a few hours, as it will be traing several machine learning models, each of which can take as long as 2 hours. Instead of waiting, we will use a model that created from a previous HPO job that ran for two days, an compared 150 models. The code below will stop the running HPO job, and then copy this model artifact to a location in S3 to simulate the final output of an HPO job. \n",
    "\n",
    "> **Note:** If you are running this lab and want to optimize on a different set of data be sure to comment out the code below to allow your hpo job to complete, and give you a customized model. Warning! This will take some time to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'stop_tuning_job'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ae49b0f7b362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhpo_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhpo_tuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'stop_tuning_job'"
     ]
    }
   ],
   "source": [
    "hpo_tuner.stop_tuning_job()\n",
    "hpo_tuner = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to wait for the HPO job to complete, call wait on the tuner. The HPO job will take many hours to complete if it was not stopped above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpo_tuner: \n",
    "    hpo_tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on complete data set\n",
    "The HPO job we ran above was not trained on the most recent data. Since we now have the best hyper-parameters, we will retrain a model on the most up to date data so any current patterns in air quality will be learned by the model. In production, we would normally skip over the hyperparameter optimization job, and just retrain our model on a daily basis prior to creating inferences. If the qaulity of our forecasts in production starts to go down, or we are aware of big changes to the data sources, for example if new countries are added, we would rerun the hpo job to get new hyper-parameters.\n",
    "\n",
    "![model training](img/training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full set of data for training\n",
    "all_data.to_json('data/all_data.json', orient='records', lines=True)\n",
    "data = dict(train= session.upload_data(path='data/all_data.json'))\n",
    "\n",
    "hpo['num_layers'] = 4 # set previous hpo range to a static value\n",
    "estimator.set_hyperparameters(**hpo)\n",
    "estimator.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy best model as an endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if endpoint_name:\n",
    "    predictor = RealTimePredictor(endpoint_name)\n",
    "    \n",
    "elif training_job_name:\n",
    "    predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.c4.2xlarge')\n",
    "\n",
    "else:\n",
    "    hpo_tuner.wait()\n",
    "    predictor = hpo_tuner.deploy(initial_instance_count=1, instance_type='ml.c4.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inferences (predictions)\n",
    "### Generate test sets to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = pd.date_range(test_start_date, periods=test_periods, freq=f'{frequency}H')\n",
    "tests = get_tests(test, test_dates, frequency, context_length, prediction_length)\n",
    "tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_predictions = False\n",
    "if generate_predictions:\n",
    "    quantile_strs = [f'0.{q}' for q in quantiles]\n",
    "    predictions = predict(endpoint_name, tests, quantile_strs)\n",
    "    predictions.to_pickle(predictions_file)\n",
    "else:\n",
    "    predictions = pd.read_pickle(predictions_file)\n",
    "\n",
    "predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictions data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_indexdb_tables()\n",
    "index_prediction_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create prediction plot\n",
    "Now we will create a plot with the actual values and the quatiles from the predictions. The initial data sources will be empty until we select a location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = filter_dates(features, graph_start_date, graph_end_date, frequency).drop(columns=['cat'])\n",
    "actuals_source = ColumnDataSource(dict(id=[], start=[], target=[])) # empty\n",
    "\n",
    "filtered_predictions = predictions.reset_index(level=1)\n",
    "predictions_source = ColumnDataSource(dict(id=[], start=[], **{f'0.{q}=[] for q in quantiles}))\n",
    "                                                               \n",
    "# create the plot\n",
    "predictions_plot = figure(\n",
    "    title='', \n",
    "    plot_width=800, plot_height=400, \n",
    "    x_axis_label='date/time', \n",
    "    y_axis_label='pm10 ',\n",
    "    x_axis_type='datetime',\n",
    "    y_range= [0, max(predictions['0.5'].max())],\n",
    "    tools=''\n",
    ")\n",
    "\n",
    "# plot vertical areas for the quantiles\n",
    "quantile_names = [f'0.{q}' for q in quantiles]\n",
    "predictions_plot.varea_stack(\n",
    "    stackers=quantile_names, \n",
    "    x='start', \n",
    "    color= inferno(len(quantiles)), \n",
    "    legend_label=quantile_names, \n",
    "    source=predictions_source,\n",
    "    alpha=1,\n",
    ")\n",
    "\n",
    "# plot actual values\n",
    "predictions_plot.line(\n",
    "    x= \"start\", y= \"target\", \n",
    "    color= 'white', \n",
    "    source= actuals_source\n",
    ")\n",
    "\n",
    "# add a legend\n",
    "predictions_plot.legend.items.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create location selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_select = Select(title='Select Location:', value=' ', options=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create prediction start slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_min = filtered_predictions.reset_index()['start'].min()\n",
    "start_slider = Slider(\n",
    "    start=0, \n",
    "    end=test_periods-1, \n",
    "    value=0, \n",
    "    step=1, \n",
    "    title=f'prediction time delta'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create javascript callback\n",
    "The javascript callback function connects all the plots and gui components together so changes will update the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_args=dict(\n",
    "    map_source= map_source, \n",
    "    actuals= actuals_source, \n",
    "    predictions= predictions_source, \n",
    "    location_select= location_select, \n",
    "    start_slider= start_slider\n",
    ")\n",
    "\n",
    "with open('javascript/map_update_callback.js', 'r') as f:\n",
    "    callback_code = f.read()\n",
    "    map_update_callback = CustomJS(code=callback_code, args=callback_args)\n",
    "    map_source.selected.js_on_change('indices', map_update_callback)\n",
    "\n",
    "with open('javascript/plot_update_callback.js', 'r') as f:\n",
    "    callback_code = f.read()    \n",
    "    plot_update_callback = CustomJS(code=callback_code, args=callback_args)\n",
    "    location_select.js_on_change('value', plot_update_callback)\n",
    "    start_slider.js_on_change('value', plot_update_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the gui\n",
    "Once we have created all the elements, we can now show the entire air quality GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(column(map_plot, location_select, predictions_plot, start_slider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Keplar.gl map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keplergl import KeplerGl \n",
    "#map_1 = KeplerGl(height=500)\n",
    "#map_1.add_data(data=predictions, name='sydney_2020')\n",
    "#map_1.save_to_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Attributions\n",
    "<p style=\"font-size: 0.9rem;font-style: italic;\"><img style=\"display: block;\" src=\"https://live.staticflickr.com/65535/49246056803_0a0bae48cf_b.jpg\" border=\"1px\" solid=\"#ddd\" border-radius=\"4px\" padding=\"5px\" width=\"150px\" alt=\"P1210668\"><a href=\"https://www.flickr.com/photos/37912374670@N01/49246056803\">\"P1210668\"</a><span> by <a href=\"https://www.flickr.com/photos/37912374670@N01\">acb</a></span> is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-sa/2.0/?ref=ccsearch&atype=html\" style=\"margin-right: 5px;\">CC BY-NC-SA 2.0</a><a href=\"https://creativecommons.org/licenses/by-nc-sa/2.0/?ref=ccsearch&atype=html\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"display: inline-block;white-space: none;margin-top: 2px;margin-left: 3px;height: 22px !important;\"><img width=\"15px\"style=\"height: inherit;margin-right: 3px;display: inline-block;\" src=\"https://search.creativecommons.org/static/img/cc_icon.svg\" /><img width=\"15px\" style=\"height: inherit;margin-right: 3px;display: inline-block;\" src=\"https://search.creativecommons.org/static/img/cc-by_icon.svg\" /><img width=\"15px\" style=\"height: inherit;margin-right: 3px;display: inline-block;\" src=\"https://search.creativecommons.org/static/img/cc-nc_icon.svg\" /><img width=\"15px\" style=\"height: inherit;margin-right: 3px;display: inline-block;\" src=\"https://search.creativecommons.org/static/img/cc-sa_icon.svg\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
